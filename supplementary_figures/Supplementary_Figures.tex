\documentclass{article}

\input{../figure-defs.tex}
\begin{document}

\inclimg{challenge1_src/images/sqanti3-eval-h1-mix.pdf}{Supplementary Fig. 1}{
  % was: Extended Data Fig. 2
  SQANTI3 evaluation of LRGASP submissions of the H1-mix dataset. Labels
  correspond to analysis tools and the color code indicates the combination of
  library preparation and sequencing platform. a) Number of gene and
  transcript detections. b) Number of Full Splice Match and Incomplete Splice
  Match transcripts. c) Number of Novel in Catalogue and Novel Not in
  Catalogue transcripts. d) Number of known and novel transcripts with full
  support at junctions and end positions. e) Percentage of transcripts with
  5´end support. f) Percentage of transcripts with 3´end support. g)
  Percentage of canonical splice junctions (SJ) and short-reads support at SJ.
  Ba: Bambu, FM: Flames, FL: FLAIR, IQ: IsoQuant, IT: IsoTools, IB: Iso_IB,
  Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST: StringTie2.
}
\inclimg{challenge1_src/images/sqanti3-eval-es.pdf}{Supplementary Fig. 2}{
  % was: Extended Data Fig. 3
  SQANTI3 evaluation of LRGASP submissions of the mouse ES dataset. Labels
  correspond to analysis tools and the color code indicates the combination of
  library preparation and sequencing platform. a) Number of gene and
  transcript detections. b) Number of Full Splice Match and Incomplete Splice
  Match transcripts. c) Number of Novel in Catalogue and Novel Not in
  Catalogue transcripts. d) Number of known and novel transcripts with full
  support at junctions and end positions. e) Percentage of transcripts with
  5´end support. f) Percentage of transcripts with 3´end support. g)
  Percentage of canonical splice junctions (SJ) and short-reads support at SJ.
  Ba: Bambu, FM: Flames, FL: FLAIR, IQ: IsoQuant, IT: IsoTools, IB: Iso_IB,
  Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST: StringTie2.
}

\inclgen{challenge1_src/generated/Extended_Fig._4_seq-depth-by-detected-features.pdf}{Supplementary Fig. 3}{
  % was: Extended Data Fig. 4.
  Relationship between sequencing depth and number of detected features. a-c) Transcripts, d-f)
  Genes.
}
\inclgen{challenge1_src/generated/Extended_Fig._5_read-length-by-detected-features.pdf}{Supplementary Fig. 4}{
  % was: Extended Data Fig. 5.
    Relationship between read length and number of detected features. a-c) Transcripts, d-f) Genes.
}
\inclgen{challenge1_src/generated/Extended_Fig._6_read-qual-by-detected-features.pdf}{Supplementary Fig. 5}{
  % was: Extended Data Fig. 6.
    Relationship between read quality and number of detected features. a-c) Transcripts, d-f) Genes.
}
\inclgen{challenge1_src/generated/Extended_Fig._7_deviance-dectected-features-by-expr-factors.pdf}{Supplementary Fig. 6}{
  % was: Extended Data Fig. 7.
    Median Absolute Deviance of detected features by experimental factor. a-c) Transcripts, d-f) Genes.
}
\inclgen{challenge1_src/generated/Extended_Fig._8_detected-trans-gene-by-tool.pdf}{Supplementary Fig. 7}{
  % was: Extended Data Fig. 8.
    Number of detected transcripts and genes per analysis tool. a-c) Transcripts, d-f) Genes.
}
\inclgen{challenge1_src/generated/Extended_Fig._15_FSM-ISM-by-plat-lib.pdf}{Supplementary Fig. 8}{
  % was: Extended Data Fig. 15.
  Number of FSM
  and ISM by sequencing platform and library preparation. a-c) FSM, d-f) ISM.
}
\inclgen{challenge1_src/generated/Extended_Fig._16_NIC-NCC-by-plat-prep.pdf}{Supplementary Fig. 9}{
  % was: Extended Data Fig. 16.
  Number of NIC
  and NNC by sequencing platform and library preparation. a-c) NIC, d-f) NNC.
}
\inclgen{challenge1_src/generated/Extended_Fig._17_FSM-by-lib-tool.pdf}{Supplementary Fig. 10}{
  % was: Extended Data Fig. 17.
  Number of FSM
  transcripts by library preparation and analysis tool. a-c) cDNA. d-f) CapTrap.
}
\inclgen{challenge1_src/generated/Extended_Fig._18_FSM-by-plat-tool.pdf}{Supplementary Fig. 11}{
  % was: Extended Data Fig. 18.
  Number of FSM
  transcripts by sequencing platform and analysis tool. a-c) PacBio, d-f) Nanopore.
}
\inclgen{challenge1_src/generated/Extended_Fig._19_ISM-by-prep-tool.pdf}{Supplementary Fig. 12}{
  % was: Extended Data Fig. 19.
  Number of ISM
  transcripts by library preparation and analysis tool. a-c) cDNA. d-f) CapTrap.
}
\inclgen{challenge1_src/generated/Extended_Fig._20_ISM-by-plat-tool.pdf}{Supplementary Fig. 13}{
  % was: Extended Data Fig. 20.
  Number of ISM
  transcripts by sequencing platform and analysis tool. a-c) Intergenic. d-f) GenicGenomic.
}
\inclgen{challenge1_src/generated/Extended_Fig._21_genic-by-plat-prep.pdf}{Supplementary Fig. 14}{
  % was: Extended Data Fig. 21.
  Number of
  Intergenic and GenicGenomic by sequencing platform and library preparation. a-c) Intergenic,
  d-f) GenicGenomic.
}
\inclgen{challenge1_src/generated/Extended_Fig._22_fusion-anti-by-plat-prep.pdf}{Supplementary Fig. 15}{
  % was: Extended Data Fig. 22.
  Number of Fusion and Antisense by sequencing platform and library preparation. a-c)
  Fusion. d-f) Antisense.
}
\inclgen{challenge1_src/generated/Extended_Fig._24_biotype-by-tool.pdf}{Supplementary Fig. 16}{
  % was: Extended Data Fig. 24.
  Distribution of Biotypes across samples. a) WTC11, c) H1-mix, c) Mouse ES.
}
\inclgen{challenge1_src/generated/Extended_Fig._25_biotype-per-pipeline.pdf}{Supplementary Fig. 17}{
  % was: Extended Data Fig. 25.
    Distribution of Biotypes across pipelines. a) WTC11, c) H1-mix, c) Mouse ES.
}
\inclgen{challenge1_src/generated/Extended_Fig._26_cnt_squanti_dist-UIC.pdf}{Supplementary Fig. 18}{
  % was: Extended Data Fig. 26.
  Number and
  SQANTI category distribution of Unique Intron Chain (UIC) consistently detected by an
  increasing number of submissions. a) H1-mix sample, b) Mouse ES sample.
}
\inclcrop{challenge1_src/images/detection-overlap-wtc11.pdf}{50pt}{Supplementary Fig. 19}{
  % was: Extended Data Fig. 27.
  Pair-wise overlap in the detection of features between pipelines; WTC11
  sample. Each value represents the feature intersection between column and
  row pipelines divided by the number of detections in the row pipeline. a)
  Genes, b) Splice junctions, c) Unique Intron Chains (UIC), c) Top UIC
  accounting for at least 50\% of the gene expression.
}
\inclcrop{challenge1_src/images/detection-overlap-h1-mix.pdf}{50pt}{Supplementary Fig. 20}{
  % was: Extended Data Fig. 28.
  Pair-wise overlap in the detection of features between pipelines; H1-mix
  sample. Each value represents the feature intersection between column and
  row pipelines divided by the number of detections in the row pipeline. a)
  Genes, b) Splice junctions, c) Unique Intron Chains (UIC), c) Top UIC
  accounting for at least 50\% of the gene expression.
}
\inclcrop{challenge1_src/images/detection-overlap-es.pdf}{55pt}{Supplementary Fig. 21}{
  % was: Extended Data Fig. 29.
  Pair-wise overlap in the detection of features between pipelines; ES mouse
  sample. Each value represents the feature intersection between column and
  row pipelines divided by the number of detections in the row pipeline. a)
  Genes, b) Splice junctions, c) Unique Intron Chains (UIC), c) Top UIC
  accounting for at least 50\% of the gene expression.
}
\inclgen{challenge1_src/generated/Extended_Fig._30_UIC-by-tool-PacBio_cDNA.pdf}{Supplementary Fig. 22}{2
  % was: Extended Data Fig. 30.
  Number of UIC detected by a tool and shared with an increasing number of other tools,
  processing PacBio_cDNA data. a) WTC11, c) H1-mix, c) Mouse ES.
}
\inclgen{challenge1_src/generated/Extended_Fig._31_UIC-by-tool-PacBio_CapTrap.pdf}{Supplementary Fig. 23}{
  % was: Extended Data Fig. 31.
  Number of UIC detected by a tool and shared with an increasing number of other tools,
  processing PacBio_CapTrap data. a) WTC11, c) H1-mix, c) Mouse ES.
}
\inclgen{challenge1_src/generated/Extended_Fig._32_UIC-by-tool-ONT_cDNA.pdf}{Supplementary Fig. 24}{
  % was: Extended Data Fig. 32.
  Number of UIC
  detected by a tool and shared with an increasing number of other tools, processing
  ONT_cDNA data. a) WTC11, c) H1-mix, c) Mouse ES.
}
\inclgen{challenge1_src/generated/Extended_Fig._33_UIC-by-tool-ONT_CapTrap.pdf}{Supplementary Fig. 25}{
  % was: Extended Data Fig. 33.
  Number of UIC
  detected by a tool and shared with an increasing number of other tools, processing
  ONT_CapTrap data. a) WTC11, c) H1-mix, c) Mouse ES.
}
\inclgen{challenge1_src/generated/Extended_Fig._34_UIC-by-tool-ONT_R2C2.pdf}{Supplementary Fig. 26}{
  % was: Extended Data Fig. 34.
  Number of UIC
  detected by a tool and shared with an increasing number of other tools, processing ONT_R2C2
  data. a) WTC11, c) H1-mix, c) Mouse ES
}
\inclgen{challenge1_src/generated/Extended_Fig._35_UIC-by-tool-ONT_dRNA.pdf}{Supplementary Fig. 27}{
  % was: Extended Data Fig. 35.
  Number of UIC
  detected by a tool and shared with an increasing number of other tools, processing ONT_dRNA
  data. a) WTC11, c) H1-mix, c) Mouse ES
}
\inclcrop{challenge1_src/generated/Extended_Fig._43_metrics-mouse-simul.pdf}{68pt}{Supplementary Fig. 28}{
  % was: Extended Data Fig. 43.
  Performance
  metrics on mouse simulated data. Sen_kn: sensitivity known transcripts, Sen_kn > 5 TPM:
  sensitivity known transcripts with expression > 5 TPM, Pre_kn: precision known transcripts,
  Sen_no: sensitivity novel transcripts, Pre_no: precision novel transcripts, 1/Red: inverse of
  redundancy.
}
\inclgen{challenge1_src/generated/Extended_Fig._44_trans-cover-real-simul.pdf}{Supplementary Fig. 29}{
  % was: Extended Data Fig. 44.
  Comparison of long-read transcript coverage between real and simulated datasets.
}
\inclcrop{challenge1_src/images/perf-known-novel.pdf}{230pt}{Supplementary Fig. 30}{
  % was: Supplementary Fig. 111.
  Performance of tools on a) genes and b) detection of curated transcript from
  manual annotation of 50 human genes manually-annotated by GENCODE.  Tools are:
  Ba: Bambu, FM: Flames, FR: FLAIR, IQ: IsoQuant, IT: IsoTools, IB: Iso_IB,
  Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST: StringTie2.
}
\inclcrop{challenge1_src/images/ES_pipeline-by-gencode-manual.pdf}{390}{Supplementary Fig. 31}{
  % was: Extended Data Fig. 47.
  Performance of tools on a) known transcript, b) novel transcripts, c) genes, and d) detection of curated transcript from
  manual annotation of 50 mouse genes manually-annotated by GENCODE.  Tools are:
  Ba: Bambu, FM: Flames, FR: FLAIR, IQ: IsoQuant, IT: IsoTools, IB: Iso_IB,
  Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST: StringTie2.
}
\inclcrop{challenge1_src/generated/Extended_Fig._48_uic-gencode-manual-by-tool.pdf}{38pt}{Supplementary Fig. 32}{
  % was: Extended Data Fig. 48.
  Detection of Unique Intron Chains (UIC) at GENCODE manual annotation loci. Ba: Bambu, FM:
  Flames, FL: FLAIR, IQ: IsoQuant, IT: IsoTools, IB: Iso_IB, Ly: LyRic, Ma: Mandalorion, TL:
  TALON-LAPA, Sp: Spectra, ST: StringTie2.
}
\inclgen{challenge1_src/images/src/summary-values/summary_figure.value.CapTrap-PacBio.pdf}{Supplementary Fig. 33}{
  Summary of performance metrics of tools for CapTrap-PacBio benchmarking
  dataset. Color scale represents the performance value ranging from worse
  (dark blue) to better (light yellow). Graphic symbol indicates the raking
  position of the tool for the metric represented in each row. SJ: Splice
  Junction, UIC: Unique Intron Chain. LO: Long (reads) Only, LS: Long and Short
  (reads), Sen_kn: Sensitivity for known transcripts, Pre_kn: Precision for
  known transcripts, Sen_no: Sensitivity for Novel transcripts, Pre_no:
  Precision for Novel transcripts, 1/Red: inverse of redundancy. Num: number,
  SRTM: Supported Reference Transcript Model, SNTM: Supported Novel Transcript
  Model, Ba: Bambu, FM: Flames, FR: FLAIR, IQ: IsoQuant, IT: IsoTools, IB:
  Iso_IB, Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST:
  StringTie2.
}
\inclgen{challenge1_src/images/src/summary-values/summary_figure.value.CapTrap-ONT.pdf}{Supplementary Fig. 34}{
  Summary of performance metrics of tools for the CapTrap-ONT benchmarking
  dataset. Color scale represents the performance value ranging from worse
  (dark blue) to better (light yellow). Graphic symbol indicates the raking
  position of the tool for the metric represented in each row. SJ: Splice
  Junction, UIC: Unique Intron Chain. LO: Long (reads) Only, LS: Long and Short
  (reads), Sen_kn: Sensitivity for known transcripts, Pre_kn: Precision for
  known transcripts, Sen_no: Sensitivity for Novel transcripts, Pre_no:
  Precision for Novel transcripts, 1/Red: inverse of redundancy. Num: number,
  SRTM: Supported Reference Transcript Model, SNTM: Supported Novel Transcript
  Model, Ba: Bambu, FM: Flames, FR: FLAIR, IQ: IsoQuant, IT: IsoTools, IB:
  Iso_IB, Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST:
  StringTie2.
}
\inclgen{challenge1_src/images/src/summary-values/summary_figure.value.R2C2-ONT.pdf}{Supplementary Fig. 35}{
  Summary of performance metrics of tools for the R2C2-ONT benchmarking
  dataset. Color scale represents the performance value ranging from worse
  (dark blue) to better (light yellow). Graphic symbol indicates the raking
  position of the tool for the metric represented in each row. SJ: Splice
  Junction, UIC: Unique Intron Chain. LO: Long (reads) Only, LS: Long and Short
  (reads), Sen_kn: Sensitivity for known transcripts, Pre_kn: Precision for
  known transcripts, Sen_no: Sensitivity for Novel transcripts, Pre_no:
  Precision for Novel transcripts, 1/Red: inverse of redundancy. Num: number,
  SRTM: Supported Reference Transcript Model, SNTM: Supported Novel Transcript
  Model, Ba: Bambu, FM: Flames, FR: FLAIR, IQ: IsoQuant, IT: IsoTools, IB:
  Iso_IB, Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST:
  StringTie2.
}
\inclgen{challenge1_src/images/src/summary-values/summary_figure.value.dRNA-ONT.pdf}{Supplementary Fig. 36}{
  Summary of performance metrics of tools for the dRNA-ONT benchmarking
  dataset. Color scale represents the performance value ranging from worse
  (dark blue) to better (light yellow). Graphic symbol indicates the raking
  position of the tool for the metric represented in each row. SJ: Splice
  Junction, UIC: Unique Intron Chain. LO: Long (reads) Only, LS: Long and Short
  (reads), Sen_kn: Sensitivity for known transcripts, Pre_kn: Precision for
  known transcripts, Sen_no: Sensitivity for Novel transcripts, Pre_no:
  Precision for Novel transcripts, 1/Red: inverse of redundancy. Num: number,
  SRTM: Supported Reference Transcript Model, SNTM: Supported Novel Transcript
  Model, Ba: Bambu, FM: Flames, FR: FLAIR, IQ: IsoQuant, IT: IsoTools, IB:
  Iso_IB, Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST:
  StringTie2.
}
\inclimg{challenge2_src/overall_irreproducibility.pdf}{Supplementary Fig. 37}{
  % was: Extended Data Fig. 54
  Overall evaluation results of irreproducibility on real data with
  multiple replicates.  The diagram illustrates the calculation of
  irreproducibility. By fitting the coefficient of variation (CV)
  versus average transcript abundance into a smooth curve, it can be
  shown that Method X has lower coefficient of variation and higher
  reproducibility.  Evaluation results of ACVC metric for different
  quantification tools and protocols-platforms. Box plots are employed
  to illustrate the five-number summary of evaluation results across
  various datasets, depicting the minimum, lower quartile, median,
  upper quartile, and maximum values.  The overall results of CV
  curves with different transcript abundances on four samples (H1-mix,
  WTC11, H1-hESC and H1-DE) with different protocols and
  platforms. Here, Bambu-merge represents the transcript
  quantification using Bambu with GENCODE plus LR-specific
  annotation. And Bambu-LR represents the transcript quantification
  using only LR-specific annotation.
}
\inclimg{challenge2_src/overall_Consistency.pdf}{Supplementary Fig. 38}{
  % was: Extended Data Fig. 55
  Overall evaluation results of consistency on real data with multiple
  replicates.  a) The diagram illustrates the calculation of
  consistency. By setting an expression threshold (i.e. 1 in this toy
  example), we can define which set of transcripts express (in blue)
  or not (in orange). This statistic is to measure the consistency of
  the expressed transcripts sets between replicates.  b) A toy example
  to show the consistency curves with different abundance
  threshold. Here, method X performs the better consistency of
  transcript abundance estimation across multiple replicates than
  method Y.  c) Evaluation results of ACC metric for different
  quantification tools and protocols-platforms. Box plots are employed
  to illustrate the five-number summary of evaluation results across
  various datasets, depicting the minimum, lower quartile, median,
  upper quartile, and maximum values.  d) The detailed evaluation
  results of consistency curves with different abundance thresholds on
  four samples (H1-mix, WTC11, H1-hESC and H1-DE) with different
  protocols and platforms.
}
\inclimg{challenge2_src/resolution_entropy_description.pdf}{Supplementary Fig. 39}{
  % was: Extended Data Fig. 56
  Resolution Entropy.  a) The software output only a few certain
  discrete values has lower resolution entropy as it cannot capture
  the continuous and subtle difference of gene expressions.  b) The
  software with continuous output values has higher resolution entropy.
}
\inclimg{challenge2_src/overall_cell_mixing_experiment.pdf}{Supplementary Fig. 40}{
  % was: Extended Data Fig. 57
  Performance evaluation on cell mixing experiment.  a) Schematic
  diagram of evaluation strategy using the cell mixing
  experiment. Here, H1-mix was initially provided for quantification
  which was a mix of H1-hESC cells and H1-DE cells at an undisclosed
  ratio. After the initial submission, the individual H1-hESC and
  H1-DE samples were released and participants submitted
  quantifications for each.  b) Evaluation results of NRMSE metric for
  different quantification tools and protocols-platforms. Bar plots
  are utilized to visualize the mean values of evaluation results
  across diverse datasets, with error bars indicating the standard
  deviation of metrics.  c) Scatter plot of expected abundance and
  observed abundance for seven participant's tools with different
  protocols and platforms.
}
\inclimg{challenge2_src/overall_SIRV-set4_data.pdf}{Supplementary Fig. 41}{
  % was: Extended Data Fig. 58
  Performance evaluation on SIRV-set 4 data.  a) Evaluation results of
  NRMSE metric for different quantification tools and
  protocols-platforms. Bar plots are utilized to visualize the mean
  values of evaluation results across diverse datasets, with error
  bars indicating the standard deviation of NRMSE metric.  b) Scatter
  plot of true abundance and estimated abundance on SIRV-set 4 data
  with different protocols and platforms.
}
\inclimg{challenge2_src/overall_simulation_data.pdf}{Supplementary Fig. 42}{
  % was: Extended Data Fig. 59
  Performance evaluation on simulation data.  a) The flow chart of
  simulation study.  b) Evaluation results of NRMSE metric for
  different quantification tools and protocols-platforms. Bar plots
  are utilized to visualize the mean values of evaluation results
  across diverse datasets, with error bars indicating the standard
  deviation of NRMSE metric.  c) Scatter plot of true abundance and
  estimated abundance on simulation data.
}
\inclimg{challenge2_src/unaccurate_annotaion_impact_on_quantification.pdf}{Supplementary Fig. 43}{
  % was: Extended Data Fig. 60
  Impact of annotation accuracy on transcript quantification. We
  assessed the performance of RSEM and LR-based tools (Bambu, FLAIR,
  FLAMES, IsoQuant, IsoTools, TALON, and NanoSim) with different
  annotations. The NRMSE metric was used to evaluate their performance
  on simulated data for human and mouse. For LR-based tools, the
  transcript quantification annotations were derived from
  sample-specific annotations identified by the participant using
  long-read RNA-seq data. As for RSEM, we present quantification
  results based on two annotations: a completely accurate annotation
  (i.e., the ground truth transcripts generated by the simulation
  data) and an inaccurate annotation (i.e., the common GENCODE
  reference annotation, which contains numerous false negative and
  false positive transcripts specific to the sample). Bar plots are
  utilized to visualize the mean values of evaluation results across
  diverse datasets, with error bars indicating the standard deviation
  of NRMSE metric.
}
\inclimgsz{overview_src/summary-of-data-edit.pdf}{140mm}{Supplementary Fig. 44}{
  % was: Supplementary Fig. 1.
  Read characteristics for the WTC-11 sample.
  a) Read lengths for the different library prep and technology combinations.
  The 500,000 longest reads for each library prep and technology combination
  fall to the right of a labeled red line overlapping each plot. b-c) The read
  identity and percent of aligned read bases of the different library prep and
  technology combinations is shown for reads of different length in 500nt
  bins.
  d) Read number for the three replicates, read identity, mismatch, and indel
  percentages for the different library prep and technology combinations are
  shown as vertical swarmplots.
  The box overlays for the swam plots percentiles are 5\%, 25\%, 50\%, 75\%, and 95\%.
}
\inclimg{challenge2_src/K-value_description.pdf}{Supplementary Fig. 45}{
  % was: Extended Data Fig. 62
  Description of K-value.  A measure of the complexity of exon-isoform
  structures for each gene.
}
\inclimg{challenge3_src/Extended_Fig._63.pdf}{Supplementary Fig. 46}{
  % was: Extended Data Fig. 63.
  Manatee genome assembly statistics. a Nanopore reads were used to obtain a
  draft genome of the Floridian manatee with Flye. The resulting assembly was
  polished with existing Illumina reads using Pilon. b BUSCO completeness.
}
\inclcrop{challenge3_src/Extended_Fig._64.pdf}{38pt}{Supplementary Fig. 47}{
  % was: Extended Data Fig. 64.
  Mapping rate of transcript detected by Challenge 3 submissions.
}
\inclimg{challenge3_src/Extended_Fig._66.pdf}{Supplementary Fig. 48}{
  % was: Extended Data Fig. 66.
  Coding potential of transcripts detected by Challenge 3 submissions.
}
\inclcrop{challenge3_src/Extended_Fig._67.pdf}{38pt}{Supplementary Fig. 49}{
  % was: Extended Data Fig. 67.
  SQANTI3 analysis of SIRV reads in manatee samples. a) SQANTI3 categories for
  reads mapping to SIRVs in cDNA-PacBio and cDNA-ONT replicates. b) Number of
  SIRV transcripts with at least one Reference Match (RM) read in cDNA-PacBio
  and cDNA-ONT replicates.
}
\inclimg{validation_src/Extended_Figure_70.pdf}{Supplementary Fig. 50}{
  % was: Extended Data  Fig. 70.
  PCR validation results for manatee isoforms for seven target
  genes (data shown in Figure 5l) broken down by the platform (ONT or PacBio)
  underlying the pipelines that led to the identification of the isoform.
}
\inclimg{validation_src/Extended_Figure_71.pdf}{Supplementary Fig. 51}{
  % was: Extended Data Fig. 71.
  Validation of ALG6 U12 Intron with WTC11 Reads. In panel (a), a novel
  transcript model, NCC_39352 (blue arrow), appears to corroborate the exon
  within the ALG6 GENCODE annotation. The mapped amplicon in the control
  junction tracks provides evidence of the preceding intron. The green arrow
  indicates the ONT and PacBio read alignment coverage over the exon, but the
  junction tracks shows a lack of support for the splice junction at the
  exon's 5' end. In panel (b), GENCODE's annotation of a rare U12 GT-AT intron
  (purple arrow), which is unsupported by minimap2. Instead, minimap2 forces a
  GT-AG intron by reporting a six-base deletion in the reference genome (red
  arrow). As all pipelines relied on minimap2, correct annotation of this
  transcript was unattainable, illustrating the challenges difficult-to-align
  regions can pose to annotation with long- read transcripts.
}
\inclimg{challenge1_src/images/read-usage-by-tool.pdf}{Supplementary Fig. 52}{
  % was: Extended Data Fig. 1.
  Read usage by analysis tool. a-c) The Percentage of Reads
  Used (PRU) is calculated as the fraction between the number of reads in
  transcript models provided in the submission of each pipelines and the
  number of available reads in the dataset. Values > 100 indicate the same
  read is assigned to more than one transcript model. Values < 100 indicate
  that not all available reads were used to predict transcript models. d)
  Distribution of the number of transcripts assigned to each long-read in the
  submitted reads2transcripts files. Values are aggregated for all submissions
  of the same tool.
}
\inclgen{challenge1_src/generated/Extended_Fig._9_detected-genes-by-plat-prep.pdf}{Supplementary Fig. 53}{
  % was: Extended Data Fig. 9.
  Number of detected genes per Platform and Library Preparation. a-c) Platform, d-f) Library
  Preparation.
}
\inclgen{challenge1_src/generated/Extended_Fig._10_detected-genes-by-plat-prep.pdf}{Supplementary Fig. 54}{
  % was: Extended Data Fig. 10.
  Number of detected transcripts per Platform and Library Preparation.
}
\inclgen{challenge1_src/generated/Extended_Fig._11_detected-trans-by-cDNA-CapTrap.pdf}{Supplementary Fig. 55}{
  % was: Extended Data Fig. 11.
  Number of
  detected transcripts in cDNA and CapTrap libraries. a-c) cDNA, d-f) CapTrap.
}
\inclgen{challenge1_src/generated/Extended_Fig._12_detected-trans-by-PacBio-ONT.pdf}{Supplementary Fig. 56}{
  % was: Extended Data Fig. 12.
  Number of
  detected transcripts in PacBio and Nanopore platforms. a-c) PacBio, d-f) Nanopore.
}
\inclgen{challenge1_src/generated/Extended_Fig._13_detected-genes-by-PacBio-ONT.pdf}{Supplementary Fig. 57}{
  % was: Extended Data Fig. 13.
  Number of
  detected genes in cDNA and CapTrap libraries. a-c) cDNA, d-f) CapTrap.
}
\inclgen{challenge1_src/generated/Extended_Fig._14_detected-genes-by-PacBio-ONT.pdf}{Supplementary Fig. 58}{
  % was: Extended Data Fig. 14.
  Number of
  detected genes in PacBio and Nanopore platforms. a-c) PacBio, d-f) Nanopore.
}
\inclgen{challenge1_src/generated/Extended_Fig._38_trans-prop-by-prep.pdf}{Supplementary Fig. 59}{
  % was: Extended Data Fig. 38.
  Properties of
  detected transcripts by library preparation. a,d,g) Length distribution. b,e,h) Exon number
  distribution. c,f,i) Counts per million
}
\inclgen{challenge1_src/generated/Extended_Fig._39_trans-prop-by-plat.pdf}{Supplementary Fig. 60}{
  % was: Extended Data Fig. 39.
  Properties of
  detected transcripts by platform. a,d,g) Length distribution. b,e,h) Exon number
  distribution. c,f,i) Counts per million
}
\inclgen{challenge1_src/generated/Extended_Fig._40_trans-by-protocol.pdf}{Supplementary Fig. 61}{
  % was: Extended Data Fig. 40.
  Properties of
  detected transcripts by experimental protocol. a,d,g) Length distribution. b,e,h) Exon number
  distribution. c,f,i) Counts per million
}
\inclgen{challenge1_src/generated/Extended_Fig._41_trans-len-by-tool.pdf}{Supplementary Fig. 62}{
  % was: Extended Data Fig. 41.
  Distribution of transcript length by analysis tool.
}
\inclgen{challenge1_src/generated/Extended_Fig._36_UIC-by-tool-sample.pdf}{Supplementary Fig. 63}{
  % was: Extended Data Fig. 36.
  Number of UIC
  consistently detected by a tool across samples. a) WTC11, c) H1-mix, c) Mouse ES
}
\inclimg{challenge1_src/images/frequent-uic-characterization.pdf}{Supplementary Fig. 64}{
  % was: Extended Data Fig. 37.
  Characterization of frequently detected UICs (FDU).  a,c,e) Structural
  category distribution of FDU. The table indicates the fold enrichment of
  each structural category within the frequently detected transcripts respect
  to their global count. b,d,f) Tools identifying FDU. The graph shows the
  enrichment in the number FDU found by a tool with respect to their global
  number of reported transcripts. The table reports the total number of FDU
  detected by the tool.
}
\inclcrop{challenge1_src/generated/Extended_Fig._49_gencode-manual-by-tool-two-datasets.pdf}{38pt}{Supplementary Fig. 65}{
  % was: Extended Data Fig. 49.
  Performance on GENCODE manually curated data. Curated transcripts selected to be present in
  at least two experimental datasets. Ba: Bambu, FM: Flames, FL: FLAIR, IQ: IsoQuant, IT:
  IsoTools, IB: Iso_IB, Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra, ST:
  StringTie2.
}
\inclcrop{challenge1_src/generated/Extended_Fig._50_gencode-manual-by-tool-three-reads.pdf}{38pt}{Supplementary Fig. 66}{
  % was: Extended Data Fig. 50.
  Performance on GENCODE manually curated data. The ground truth is the set of manually
  annotated transcripts with more than two reads. Ba: Bambu, FM: Flames, FL: FLAIR, IQ:
  IsoQuant, IT: IsoTools, IB: Iso_IB, Ly: LyRic, Ma: Mandalorion, TL: TALON-LAPA, Sp: Spectra,
  ST: StringTie2.
}
\inclgenb{challenge1_src/generated/Extended_Fig._51_gencode-manual-by-prep.pdf}{Supplementary Fig. 67}{
  % was: Extended Data Fig. 51.
 Performance on GENCODE manually curated data by Library Preparation.
}
\inclgenb{challenge1_src/generated/Extended_Fig._52_gencode-manual-by-plat.pdf}{Supplementary Fig. 68}{
  % was: Extended Data Fig. 52.
  Performance on GENCODE manually curated data by Platform.
}
\inclimg{validation_src/Extended_Data_Fig69_transcript_lengths_by_validation_status.pdf}{Supplementary Fig. 69}{
  % was: Extended Data Fig. 69.
  The distribution of lengths corresponding to the target
  transcript isoform across the entire validation experiment (including
  GENCODE, Platform, and Consistency groups), broken down by their validation
  status.
}
\inclimgsm{overview_src/challenge-submission.pdf}{Supplementary Fig. 70}{
  % was: Supplementary Fig. 2.
  Challenge submission. a) Overview of submissions to Challenges 1 and 2. Each
  entry was derived from a specific data category, library prep, and
  sequencing platform combination. All available samples for the selected
  combination must be included in an entry. b) Overview of submissions for
  Challenge 3.
}
\inclimgsm{overview_src/challenge1_overview_v1.pdf}{Supplementary Fig. 71}{
  % was: Supplementary Fig. 8.
  Flow diagram of Challenge 1: Transcript isoform detection with a
  high-quality genome. Samples, library prep methods, and sequencing platforms
  used in the challenge are indicated at the top. Participants select which
  data category, library prep, and sequencing platform to analyze, run their
  pipelines to generate transcript predictions, and submit an entry which
  includes predictions for all samples. The entries include a GTF file of the
  transcript models and a TSV file that assigns reads that supported each
  transcript model.
}
\inclcropsz{overview_src/challenge2_overview_v1.pdf}{120pt}{150mm}{Supplementary Fig. 72}{
  % was: Supplementary Fig. 9.
  Flow diagram of Challenge 2: Transcript isoform quantification. Samples,
  library prep methods, and sequencing platforms used in the challenge are
  indicated at the top. Participants select which data category, library prep,
  and sequencing platform to analyze, run their pipelines to generate
  transcript predictions, and submit an entry which includes predictions for
  all samples. The entries include a GTF file of the transcript models that
  are quantified and a TSV file of the expression quantification. The H1 and
  endodermal cell samples were released after the initial submission deadline
  and participants were required to submit the quantification after the
  deadline.
}
\inclimgsm{overview_src/challenge3_overview_v1.pdf}{Supplementary Fig. 73}{
  % was: Supplementary Fig. 10.
  Flow diagram of Challenge 3. Samples, library prep methods, and sequencing
  platforms used in the challenge are indicated at the top. Participants
  select which data category and sequencing platform to analyze, run their
  pipelines to generate transcript predictions, and submit an entry which
  includes predictions for all samples. The entries include a FASTA file of
  the transcript models and a TSV file that assigns reads that supported each
  transcript model.
}
\inclimg{overview_src/submission-structure.pdf}{Supplementary Fig. 74}{
  % was: Supplementary Fig. 3.
  Schematic of directory structure and files that are included in each entry.
}
\inclimgsm{overview_src/challenge1_evaluation_v1.pdf}{Supplementary Fig. 75}{
  % was: Supplementary Fig. 11.
  Flow diagram of the evaluation for Challenge 1. Benchmarks and additional
  orthogonal data that was used for the evaluation are indicated. For example,
  CAGE and QuantSeq data from WTC11 cells were generated and made available
  only after participant submissions; therefore, they represent “hidden”
  data. These was used to define 5’ transcript starts and 3’ ends.
}
\inclimgsz{overview_src/challenge2_evaluation_v2.pdf}{170mm}{Supplementary Fig. 76}{
  % was: Supplementary Fig. 12.
  Flow diagram of the evaluation for Challenge 2. a) Evaluation of Challenge
  2 can be separated into metrics when a ground truth is known or a ground
  truth is unknown. b) Example analyses to evaluate transcript expression
  using the cell mixing experiment. A sample, H1_mix, was initially provided
  for quantification which was a mix of H1 cells and endodermal cells at an
  undisclosed ratio. After the initial submission, the individual H1 and
  endodermal cell samples were released and participants submitted
  quantifications for each.
}
\inclimg{overview_src/challenge3_evaluation_v1.pdf}{Supplementary Fig. 77}{
  % was: Supplementary Fig. 13.
  Flow diagram of the evaluation for Challenge 3. Only SIRVs are available for
  ground truth information. The evaluation was based on a comparative
  assessment of the predictions followed by targeting specific candidates for
  further validation.
}
\inclimgsz{validation_src/LRGASP_Experimental_Validation_plan.pdf}{140mm}{Supplementary Fig. 78}{
  % was: Supplementary Fig. 14.
  Experimental validation approaches for the LRGASP challenges. (A) Multiple
  categories of types of transcript were selected for validation (shown in
  green boxes). These loci will be viewed in the UCSC Genome Browser along
  with additional datasets to aid in the manual design of primers. Amplicons
  will be analyzed by fragment size and pooled to perform long-read sequencing
  with PacBio and ONT (B) A select number of genes were selected for
  transcript isoform-specific qPCR. A combination of probes detecting
  constitutive and alternative regions will be used. (C) RT-PCR validation
  will be performed similar to Challenge 1, except transcript were selected
  from well-studied mammalian immune-related genes.
}
\inclimg{validation_src/primer-design-example.pdf}{Supplementary Fig. 79}{
  % was: Supplementary Fig. 15.
  Designing validation primers. a) An example of a unique intron in transcript
  NNC_381534 to validation.  The green and blue region vertical highlights
  indicate the manually selected primer pair regions.  The 'Targets' track,
  produced by Primers-Juju, recapitulates the region as blue item B2M+1, and
  transcript with the maximal possible amplicon drawn in thick boxes. b) The
  Primers-Juju track hub with the addition of the primer pairs design. This
  adds Primer3 results (Primers track) and the most stable primer along with
  the amplicon sequence for the target transcript (Amplicons track).
}

\end{document}
